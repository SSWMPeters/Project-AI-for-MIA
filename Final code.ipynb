{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.losses import mse\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Conv2D, Flatten, Reshape, Conv2DTranspose\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name is CVEA_Project_AI_version_1, class 0\n",
      "Parameters-->\n",
      " Number of epochs: 15\n",
      " Batch size: 32\n",
      " Validation split: 0.1\n",
      " Latend dim: 100\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory where the dataset is located\n",
    "base_dir = 'C:\\\\Users\\\\20203894\\\\Documents\\\\8p361'\n",
    "\n",
    "# Define the model name\n",
    "model_name = 'CVEA_Project_AI_version_'+'1'\n",
    "\n",
    "# Define the file path for saving the model architecture\n",
    "model_filepath = model_name + '.json'\n",
    "\n",
    "# Define the file path for saving the model weights\n",
    "weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 32\n",
    "\n",
    "# Image size\n",
    "img_size = 96\n",
    "\n",
    "# The number of epochs for training\n",
    "nr_epochs = 15\n",
    "\n",
    "# %\n",
    "split = 0.10\n",
    "\n",
    "# The class type (assuming binary classification)\n",
    "class_type = 0\n",
    "\n",
    "# latent dimensions for the VAE\n",
    "latent_dim = 100\n",
    "\n",
    "# Number of images displayed\n",
    "n = 1000 \n",
    "\n",
    "print('Model name is {}, class {}'.format(model_name, class_type))\n",
    "print('Parameters-->\\n Number of epochs: {}\\n Batch size: {}\\n Validation split: {}\\n Latend dim: {}'.format(nr_epochs, batch_size, split, latent_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_percentage=0.2, batch_size_gen=batch_size, class_type=class_type, split=split, img_size=img_size):\n",
    "    \"\"\"\n",
    "    Uses the ImageDataGenerator function from the Keras API to return images in batches,\n",
    "    train_gen for the training data and val_gen for the validation data.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Base directory containing the dataset.\n",
    "        train_percentage (float): Percentage of training data to be used. Default is 0.2.\n",
    "        batch_size_gen (int): Batch size for data generators. Default is 32.\n",
    "        class_type (int): Class type to be used. Default is 0.\n",
    "        split (float): Fraction of training data to be used as validation data. Default is 0.2.\n",
    "        img_size (int): Size of the images. Default is 96.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing train_gen and val_gen, both are generators.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dataset parameters\n",
    "    train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "    valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "    RESCALING_FACTOR = 1./255\n",
    "    \n",
    "    # Instantiate data generators\n",
    "    datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "\n",
    "    # Generate data batches for training and validation sets\n",
    "    train_gen = datagen.flow_from_directory(train_path,\n",
    "                                             target_size=(img_size, img_size),\n",
    "                                             batch_size=batch_size_gen,\n",
    "                                             subset='training',\n",
    "                                             classes=[str(class_type)],\n",
    "                                             class_mode='input')\n",
    "\n",
    "    # Calculate the number of samples for training data based on train_percentage\n",
    "    num_train_samples = int(train_percentage * len(train_gen))\n",
    "    \n",
    "    # Extract desired percentage of training data for training\n",
    "    train_gen_subset = train_gen\n",
    "\n",
    "    val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                           target_size=(img_size, img_size),\n",
    "                                           batch_size=batch_size_gen,\n",
    "                                           subset='validation',\n",
    "                                           class_mode='input',\n",
    "                                           classes=[str(class_type)],\n",
    "                                           shuffle=False)\n",
    "    return train_gen_subset, val_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, train_percentage=0.2, batch_size_gen=batch_size, class_type=class_type, split=split, img_size=img_size):\n",
    "    \"\"\"\n",
    "    Uses the ImageDataGenerator function from the Keras API to return images in batches,\n",
    "    train_gen for the training data and val_gen for the validation data.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Base directory containing the dataset.\n",
    "        train_percentage (float): Percentage of training data to be used. Default is 0.2.\n",
    "        batch_size_gen (int): Batch size for data generators. Default is 32.\n",
    "        class_type (int): Class type to be used. Default is 0.\n",
    "        split (float): Fraction of training data to be used as validation data. Default is 0.2.\n",
    "        img_size (int): Size of the images. Default is 96.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing train_gen and val_gen, both are generators.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dataset parameters\n",
    "    train_path = os.path.join(base_dir, 'train+val', 'train')\n",
    "    valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "    RESCALING_FACTOR = 1./255\n",
    "    \n",
    "    # Instantiate data generators\n",
    "    datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "\n",
    "    # Generate data batches for training and validation sets\n",
    "    train_gen = datagen.flow_from_directory(train_path,\n",
    "                                                   target_size=(img_size, img_size),\n",
    "                                                   batch_size=batch_size_gen,\n",
    "                                                   subset='training',\n",
    "                                                   classes=[str(class_type)],\n",
    "                                                   class_mode='input')\n",
    "\n",
    "    # # Calculate the number of samples for training data based on train_percentage\n",
    "    # num_train_samples = int(train_percentage * len(train_gen))\n",
    "    \n",
    "    # # Extract desired percentage of training data for training\n",
    "    # train_gen_subset = next(train_gen)[:num_train_samples]\n",
    "\n",
    "    val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                                target_size=(img_size, img_size),\n",
    "                                                batch_size=batch_size_gen,\n",
    "                                                subset='validation',\n",
    "                                                class_mode='input',\n",
    "                                                classes=[str(class_type)],\n",
    "                                                shuffle=False)\n",
    "    return train_gen, val_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_gen, val_gen \u001b[38;5;241m=\u001b[39m get_pcam_generators(base_dir, batch_size_gen\u001b[38;5;241m=\u001b[39mbatch_size, class_type\u001b[38;5;241m=\u001b[39m\u001b[43mi\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(val_gen)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "train_gen, val_gen = get_pcam_generators(base_dir, batch_size_gen=batch_size, class_type=i)\n",
    "\n",
    "\n",
    "print(val_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_vae(train_gen, val_gen, weights_filepath, model_name, nr_epochs=1, latent_dim=latent_dim, batch_size=batch_size, img_size=img_size):\n",
    "    \n",
    "    # Define input shape and latent dimension\n",
    "    input_shape = (img_size, img_size, 3)\n",
    "    \n",
    "    # Encoder network\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    shape_before_flattening = K.int_shape(x)\n",
    "    x = Flatten()(x)\n",
    "    z_mean = Dense(latent_dim)(x)\n",
    "    z_log_var = Dense(latent_dim)(x)\n",
    "    \n",
    "\n",
    "    # Sampling function\n",
    "    @tf.function\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    \n",
    "\n",
    "    # Reparameterization trick\n",
    "    z = Lambda(sampling)([z_mean, z_log_var])\n",
    "    \n",
    "\n",
    "    # Decoder network\n",
    "    decoder_input = Input(K.int_shape(z)[1:])\n",
    "    x = Dense(np.prod(shape_before_flattening[1:]), activation='relu')(decoder_input)\n",
    "    x = Reshape(shape_before_flattening[1:])(x)\n",
    "    x = Conv2DTranspose(128, (2, 2), activation='relu', padding='same', )(x)\n",
    "    x = Conv2DTranspose(64, (2, 2), activation='relu', padding='same', strides=(2, 2))(x)\n",
    "    x = Conv2DTranspose(32, (2, 2), activation='relu', padding='same', )(x)\n",
    "    x = Conv2DTranspose(16, (2, 2), activation='relu', padding='same', )(x)\n",
    "    x = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    # Define the VAE model\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    decoder = Model(decoder_input, x, name='decoder')\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs, outputs, name=model_name)\n",
    "\n",
    "    # Define the VAE loss function\n",
    "    reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1] * input_shape[2]\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=1)\n",
    "    B = 1000   \n",
    "    vae_loss = K.mean(B * reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.add_metric(kl_loss, name=\"kl_loss\")\n",
    "    vae.add_metric(reconstruction_loss, name=\"reconstruction_loss\")\n",
    "    vae.compile(optimizer='adam')\n",
    "\n",
    "    # Serialize model to JSON\n",
    "    model_json = vae.to_json() # serialize model to JSON\n",
    "    with open(model_name, 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    # Define EarlyStopping callback to stop training when the model stops improving\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience = 5,\n",
    "                                  restore_best_weights=True)\n",
    "    \n",
    "    # Define other callbacks\n",
    "    checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    tensorboard = TensorBoard(log_dir=os.path.join('logs', model_name))\n",
    "    callbacks_list = [early_stopping, checkpoint, tensorboard]\n",
    "\n",
    "    # Calculate number of steps per epoch for training and validation sets\n",
    "    train_steps = train_gen.n // train_gen.batch_size\n",
    "    val_steps = val_gen.n // val_gen.batch_size\n",
    "    \n",
    "    # Fit the VAE model\n",
    "    vae.fit(train_gen, \n",
    "            steps_per_epoch=train_steps, \n",
    "            epochs=nr_epochs, \n",
    "            batch_size=batch_size, \n",
    "            validation_data=val_gen, \n",
    "            validation_steps=val_steps,\n",
    "            callbacks=callbacks_list)\n",
    "    \n",
    "    # Returns the trained VAE model\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_img(vae, val_gen, img_size=img_size):\n",
    "    \n",
    "    # Reconstruct images using the trained VAE model\n",
    "    decoded_imgs = vae.predict(val_gen)\n",
    "    \n",
    "    \n",
    "    # Display the original and reconstructed images\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        \n",
    "        # Display the original image\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        img, label = val_gen.next()\n",
    "        plt.imshow(img[0])\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # Display the reconstructed image\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_imgs[i].reshape(img_size, img_size,3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def generate_new_img(vae, base_dir, class_type=0, num_samples=10, delete_files=True, img_size=img_size):\n",
    "    \n",
    "    # Generate new images using random latent vectors\n",
    "    random_latent_vectors  = np.random.random((num_samples, img_size, img_size, 3))\n",
    "    decoded_imgs = vae.predict(random_latent_vectors)\n",
    "    \n",
    "    # Path to the directory where you want to save the images\n",
    "    save_dir = base_dir + \"/train_new_images/{}/\".format(str(class_type))\n",
    "\n",
    "    # Create the save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Optionally delete existing files in the directory\n",
    "    for file in os.listdir(save_dir):\n",
    "        if file.endswith('.jpg') and delete_files == True:\n",
    "            os.remove(save_dir + file)\n",
    "\n",
    "    # Iterate through generated images and save them\n",
    "    for i in range(len(decoded_imgs)):\n",
    "        img = decoded_imgs[i].reshape(img_size, img_size,3)\n",
    "        \n",
    "        # Generate a random name for the image\n",
    "        random_name = ''.join(random.choices('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ', k=10))\n",
    "        \n",
    "        # Save the image with the random name and jpg extension\n",
    "        plt.imsave(os.path.join(save_dir, random_name + \".jpg\"), img)\n",
    "\n",
    "    print(\"Images saved to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72000 images belonging to 1 classes.\n",
      "Found 0 images belonging to 1 classes.\n",
      "Epoch 1/15\n",
      "   2/2250 [..............................] - ETA: 3:17:05 - loss: 2783876.0000 - kl_loss: 3278.4658 - reconstruction_loss: 2780.5977"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m train_gen, val_gen \u001b[38;5;241m=\u001b[39m get_pcam_generators(base_dir, batch_size_gen\u001b[38;5;241m=\u001b[39mbatch_size, class_type\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Construct VAE model and train it\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m vae \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_vae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Reconstruct and display images\u001b[39;00m\n\u001b[0;32m     11\u001b[0m reconstruct_img(vae, val_gen)\n",
      "Cell \u001b[1;32mIn[23], line 76\u001b[0m, in \u001b[0;36mconstruct_vae\u001b[1;34m(train_gen, val_gen, weights_filepath, model_name, nr_epochs, latent_dim, batch_size, img_size)\u001b[0m\n\u001b[0;32m     73\u001b[0m val_steps \u001b[38;5;241m=\u001b[39m val_gen\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m val_gen\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Fit the VAE model\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnr_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Returns the trained VAE model\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vae\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\.conda\\envs\\8p361\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform autoencoder for specified classes (for both classes = [0,1])\n",
    "classes = [0,1]\n",
    "for i in classes:\n",
    "    # Get data generators for the specified class\n",
    "    train_gen, val_gen = get_pcam_generators(base_dir, batch_size_gen=batch_size, class_type=i)\n",
    "    \n",
    "    # Construct VAE model and train it\n",
    "    vae = construct_vae(train_gen, val_gen, weights_filepath, model_name, nr_epochs, latent_dim)\n",
    "    \n",
    "    # Reconstruct and display images\n",
    "    reconstruct_img(vae, val_gen)\n",
    "    \n",
    "    # Generate new images and save them\n",
    "    generate_new_img(vae, base_dir, class_type=i, num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for the CNN \n",
    "#the pathway for the train+val data\n",
    "path = 'C:\\\\Users\\\\20213314\\\\Documents\\\\BMT\\\\Jaar 3\\\\Project AI\\\\Data'\n",
    "# save the model and weights\n",
    "model_name = 'cnn_model_group_final'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcam_generators(base_dir, batch_size_gen=batch_size, class_type=class_type, split=split, img_size=img_sizebase_dir, img_size=img_size,  train_batch_size=5, val_batch_size=32):\n",
    "    \"\"\"\n",
    "    Uses the ImageDataGenerator function from the Keras API to return images in batches,\n",
    "    train_gen for the training data and val_gen for the validation data.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Base directory containing the dataset.\n",
    "        train_batch_size (int): Batch size for training data. Default is 32.\n",
    "        val_batch_size (int): Batch size for validation data. Default is 32.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing train_gen and val_gen, both are generators.\n",
    "    \"\"\"\n",
    "    # Dataset parameters\n",
    "    train_path = os.path.join(base_dir, 'train_new_images')#, 'train')\n",
    "    valid_path = os.path.join(base_dir, 'train+val', 'valid')\n",
    "    RESCALING_FACTOR = 1./255\n",
    "    \n",
    "    # Instantiate data generators\n",
    "    datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "\n",
    "    # Generate data batches for training and validation sets\n",
    "    train_gen = datagen.flow_from_directory(train_path,\n",
    "                                            target_size=(imgIMAGE_SIZE, IMAGE_SIZE),\n",
    "                                            batch_size=train_batch_size,\n",
    "                                            class_mode='binary')\n",
    "\n",
    "    val_gen = datagen.flow_from_directory(VALID_PATH,\n",
    "                                          target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                          batch_size=val_batch_size,\n",
    "                                          class_mode='binary',\n",
    "                                          shuffle=False)\n",
    "    \n",
    "    return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(kernel_size=(3,3), pool_size=(4,4), first_filters=32, second_filters=64):\n",
    "    \"\"\"\n",
    "    Generates a convolutional neural network model with configurable parameters.\n",
    "\n",
    "    Args:\n",
    "        kernel_size (tuple): Tuple specifying the height and width of the 2D convolution window. Default is (3, 3).\n",
    "        pool_size (tuple): Tuple specifying the factor by which to downscale in the pooling operation. Default is (4, 4).\n",
    "        first_filters (int): Number of filters in the first convolutional layer. Default is 32.\n",
    "        second_filters (int): Number of filters in the second convolutional layer. Default is 64.\n",
    "\n",
    "    Returns:\n",
    "        model: A Keras Sequential model.\n",
    "    \"\"\"\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    model.add(Conv2D(first_filters, kernel_size, activation='relu', padding='same', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "    model.add(MaxPool2D(pool_size=pool_size))\n",
    "\n",
    "    model.add(Conv2D(second_filters, kernel_size, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(pool_size=pool_size))\n",
    "\n",
    "    # Flatten the output to feed into dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add dense layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(SGD(learning_rate=0.01, momentum=0.95), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(),'accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_gen, val_gen, weights_filepath, model_name, epochs=1):\n",
    "    \"\"\"\n",
    "    Trains the provided model using the given data generators and saves the best weights.\n",
    "\n",
    "    Args:\n",
    "        model: The Keras Sequential model to train.\n",
    "        train_gen: Generator for training data.\n",
    "        val_gen: Generator for validation data.\n",
    "        weights_filepath (str): Filepath to save the best weights.\n",
    "        model_name (str): Name of the model, used for TensorBoard logging.\n",
    "        epochs (int): Number of epochs to train the model for. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "        history: History object containing training metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    model_json = model.to_json() # serialize model to JSON\n",
    "    with open(model_name, 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # Define callbacks\n",
    "    checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    tensorboard = TensorBoard(log_dir=os.path.join('logs', model_name))\n",
    "    callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "    # Train the model\n",
    "    train_steps = train_gen.n // train_gen.batch_size\n",
    "    val_steps = val_gen.n // val_gen.batch_size\n",
    "\n",
    "    history = model.fit(train_gen, steps_per_epoch=train_steps, \n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=val_steps,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks_list)\n",
    "    return history\n",
    "\n",
    "def get_fcn_model(first_filters=32, second_filters=64, kernel_size=(3,3), pool_size=(4,4)):\n",
    "    \"\"\"\n",
    "    Generates a fully convolutional neural network model with configurable parameters.\n",
    "\n",
    "    Args:\n",
    "        first_filters (int): Number of filters in the first convolutional layer. Default is 32.\n",
    "        second_filters (int): Number of filters in the second convolutional layer. Default is 64.\n",
    "        kernel_size (tuple): Tuple specifying the height and width of the 2D convolution window. Default is (3, 3).\n",
    "        pool_size (tuple): Tuple specifying the factor by which to downscale in the pooling operation. Default is (4, 4).\n",
    "\n",
    "    Returns:\n",
    "        model: A Keras Sequential model.\n",
    "    \"\"\"\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    model.add(Conv2D(first_filters, kernel_size, activation='relu', padding='same', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "    model.add(MaxPool2D(pool_size=pool_size))\n",
    "\n",
    "    model.add(Conv2D(second_filters, kernel_size, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(pool_size=pool_size))\n",
    "\n",
    "    model.add(Conv2D(second_filters, kernel_size, activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(pool_size=pool_size))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Conv2D(1, kernel_size, activation='sigmoid', padding='same'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(SGD(learning_rate=0.01, momentum=0.95), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(),'accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_and_auc(model, val_gen):\n",
    "    \"\"\"\n",
    "    Performs the calculations necessary for the ROC curve and its AUC value.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras model.\n",
    "        val_gen: Generator for validation data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: False positive rate, true positive rate, and AUC value.\n",
    "    \"\"\"\n",
    "    # Predict probabilities for the validation set\n",
    "    predictions = model.predict(val_gen)\n",
    "\n",
    "    # Calculate the false positive rate (FPR) and true positive rate (TPR)\n",
    "    fpr, tpr, thresholds = roc_curve(val_gen.labels, predictions)\n",
    "    \n",
    "    # Calculate the area under the ROC curve (AUC)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Print classification report\n",
    "    predictions[predictions <= 0.5] = 0.\n",
    "    predictions[predictions > 0.5] = 1.\n",
    "    print('Classification report:\\n', classification_report(val_gen.labels, predictions))\n",
    "    print('Confusion matrix:\\n', confusion_matrix(val_gen.labels, predictions))\n",
    "    \n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, roc_auc, type='dense'):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve using the false positive rate, true positive rate, and AUC value.\n",
    "\n",
    "    Args:\n",
    "        fpr (array): False positive rate.\n",
    "        tpr (array): True positive rate.\n",
    "        roc_auc (float): Area under the ROC curve.\n",
    "        type (str): Type of model to use ('dense' or 'conv'). Default is 'dense'. \n",
    "    \"\"\"\n",
    "    # Plot the ROC curve\n",
    "    if type == 'dense':\n",
    "        name_type ='connected'\n",
    "    elif type == 'conv':\n",
    "        name_type='convolutional'\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='gray', lw=2, label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve of model with fully {} layers\".format(name_type))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def calculate_and_plot_ROC_AUC(type='dense', model_name='my_model'):\n",
    "    \"\"\"\n",
    "    Combine different functions to create and train the model, and to calculate and plot the ROC curve and AUC value.\n",
    "\n",
    "    Args:\n",
    "        type (str): Type of model to use ('dense' or 'conv'). Default is 'dense'.\n",
    "        model_name (str): Name of the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get the model\n",
    "    if type == 'dense':\n",
    "        model = get_model()\n",
    "    elif type == 'conv':\n",
    "        model = get_fcn_model()\n",
    "\n",
    "    print('Summary of model:')\n",
    "    for layer in model.layers:\n",
    "        print(layer.output_shape)\n",
    "\n",
    "    # Get the data generators\n",
    "    train_gen, val_gen = get_pcam_generators(path)\n",
    "    \n",
    "    model_filepath = model_name + '.json'\n",
    "    weights_filepath = model_name + '_weights.hdf5'\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_gen, val_gen, weights_filepath, model_filepath, epochs=3)\n",
    "\n",
    "    # Load the trained model weights\n",
    "    model.load_weights(weights_filepath)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(val_gen)\n",
    "    print(\"Loss:\", score[0])\n",
    "    print(\"Accuracy:\", score[1])\n",
    "    \n",
    "    # Calculate ROC and AUC\n",
    "    fpr, tpr, roc_auc = calculate_roc_and_auc(model, val_gen)\n",
    "    print(\"AUC:\", roc_auc)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plot_roc_curve(fpr, tpr, roc_auc, type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
